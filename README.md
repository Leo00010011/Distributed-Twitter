# <center>Tweeter Distribu칤do 游님</center>

## Equipo:
- L치zaro Daniel Gonz치lez Mart칤nez
- Alejandra Monz칩n Pe침a
- Leonardo Ulloa Ferrer

## Funcionalidades del Sistema

Dweeter (Distributed Tweeter), es una red social que permite a los usuarios compartir textos en sus perfiles y otra operaciones similares a las que se pueden realizar en la bien conocida aplicaci칩n de Tweeter. 

Las acciones que pueden realizar los usuarios son: 

>- Registrarse en el sistema
>- Iniciar Sesi칩n
>- Publicar un Dweet
>- Re-publicar un Dweet
>- Seguir a otro usuario
>- Ver el perfil de alg칰n usuario
>- Pedir nuevos Dweets
>- Cerrar sesi칩n

#### Registrarse en el sistema 

Para registrarse el usuario debe proveer un `Nombre`, un `Nick` que ha de ser 칰nico y por el que se le identificar치 por los restantes usuarios para poder seguirlo y ver su perfil, y una `Contrase침a`. 

La acci칩n de un usuario de registrarse ser치 exitosa si el `Nick` que escoja no est치 en uso, en caso contrario recibir치 un mensaje inform치ndole que debe buscar otro Nick.

#### Iniciar Sesi칩n 
La acci칩n de iniciar sesi칩n requiere que el usuario est칠 registrado en el sistema. Para loggearse el usuario debe poner su `Nick` y `Contrase침a`, si los datos son introducidos correctamente el loggeo ser치 exitoso, en caso que la contrase침a no se corresponda con el Nick o que el Nick no est칠 registrado se informar치 al usuario con un mensaje de combinaci칩n de Nick/Contrase침a incorrecta.

Para poder publicar, seguir, re-publicar, ver perfiles y pedir visualizar Dweets el usuario debe estar loggeado en el sistema.

#### Publicar Dweet 
Publicar un Dweet requiere que el usuario introduzca el texto de la publicaci칩n, el cual no debe exceder los 225 caracteres.

Si el usuario est치 loggeado y el Dweet cumple las restricciones de tama침o, entonces se publicar치 el Dweet y este se agregar치 a su perfil.

#### Re-publicar un Dweet 
La acci칩n de re-publicar un Dweet requiere que el usuario seleccione un Dweet existente, cuando est칠 viendo las publicaciones de otros usuarios si desea que esta se agregue a su perfil y que sea vista por sus seguidores puede optar por hacer un Re-Dweet.

#### Seguir un usuario

Un usuario para seguir a otro debe conocer su `Nick` y decir que quiere comenzar a seguir a ese usuario, si el Nick del usuario al que quiere seguir existe, comenzar치 a seguir a este usuario. Seguir a un usuario implica que cuando se pida ver nuevos Dweets las publicaciones y re-publicaciones de este usuario van a eventualmente aparecer, manteni칠ndose al tanto de su contenido.

#### Ver perfil
Para ver un perfil se debe introducir el `Nick` del usuario cuyo perfil se desea ver, esto mostrar치 todos los Dweets y ReDweets hechos por este usuario. 

#### Pedir nuevos Dweets
Esta acci칩n muestra un conjuto de Dweets y ReDweets de usuarios a los que est치s siguiendo, es una acci칩n que mientras mas veces se repita m치s cantidad de contenido se podr치 visualizar e se informar치 de publicaciones que a칰n no se hayan visto.

#### Cerrar Sesi칩n 

Es la forma segura de salir de la red social y que nadie pueda acceder a tu cuenta a menos que conozca el `Nick` y `Contrase침a` correspondientes para loggearse nuevamente.

### Interacci칩n con Dwitter

El usuario para poder realizar todas las acciones disponibles utilizar치 una consola interactiva de apariencia sencilla y f치cil uso.

## Almacenamiento de informaci칩n

Para almacenar la informaci칩n se utiliz칩 una base de datos relacional en SQLite, el dise침o de la misma y las operaciones de inserci칩n, consulta, eliminaci칩n de datos se manej칩 con la librer칤a `Peewee` de `Python`, que funciona muy similar a la ORM de Django.

### Almacenamiento distribu칤do

Como los servidores de datos (`TweeterServers`) est치n dispuestos en forma de anillo y se comunican para buscar los recursos mediante una `DHT` con un algoritmo `Chord`, entonces se aprovecha el identificador de cada Nodo del Chord para repartir la informaci칩n a almacenar. Como en el algoritmo Chord cada Nodo responde por los recursos que tengan identificador menor que el de dicho Nodo y mayor que el de su antecesor en la DHT, y aprovechando las caracter칤sticas propias de las Redes Sociales, en que todo requiere de un usuario registrado para poder intercambiar, utilizamos la misma funci칩n de `hash` con la que se decidi칩 el identificador del Nodo para hashear los Nicks de los usuarios, de modo que todos los recursos (publicaciones, Nicks de a quienes sigue, Datos de loggeo, etc) relativos a usuarios con identificador en el mismo segmento del chord se encuentran en un mismo Nodo; mientras que usuarios con identifiador en dos secciones diferentes del anillo del chord tienen sus datos almacenados en Nodos diferentes. 

Con esta forma de distribuir los datos, mientras m치s crezca el n칰mero de usuarios de Dweeter, mayor cantidad de Nicks de usuario existir치n y m치s equilibrada estar치 la cantidad de informaci칩n almacenada en cada uno de los Nodos.

## Componentes y arquitectura de la Red

En el sistema distribu칤do Dweeter, exiten 4 componentes principales, cada una con un comportamiento y funcionalidades espec칤ficas en la red.

>-  Client
>-  EntryServer
>-  ChordServer
>-  TweeterServer

Cada computadora de la red tiene alguno de estos comportamientos asignados y en base a esto se establecen los protocolos de comunicaci칩n entre ellos para el manejo de datos.

Para `EntryServer`, `ChordServer` y `TweeterServer`, los cuales son realmente los servidores fundamentales de la red, se tiene que para reponder a peticiones utilizan una piscina de hilos, es decir tienen el comportamiento de un `MultiThreadedServer`, el cual describiremos detalladamente en pr칩ximas secciones.

A continuaci칩n explicamos con mayor detalle las funcionalidades y comportamiento de cada componente.


#### Client

Ofrece la funcionalidades para establecer la comunicaci칩n con los servicios generales del Dweeter, desde la perspectiva del cliente o consumidor. Es la componente m치s cercana al usuario.


#### EntryServer

Es la componente intermedia entre `Client`, `ChordServer`, y `TweeterServer`. Se encarga de recepcionar las peticiones del `Client`, reconocerlas, validarlas y hacer la petici칩n al `TweeterServer`. Sirve para velar en parte por la seguridad del Sistema, al obligar la comuniaci칩n `Client-EntryServer` y `EntryServer-TweeterServer`, lo implica que el `Client` aunque quiera comunicarse con el `EntryServer` no podr칤a hacerlo, o al menos no de forma trivial. Por otra parte al servir de mediadora, est치 capacitada para percibir cu치ndo hay un fallo en la red y notificar al `Client`. Adem치s la interacci칩n con `ChordServer` es la m칤nima necesaria para introducirlo al anillo del Chord, aclarando que realmente NO es el `EntryServer` quien decide d칩nde va el nuevo `ChordServer` en el anillo, sino que a este se le da el "contacto" de alg칰n otro `ChordServer` ya insertado, y haciendo el algoritmo de Chord, este se podr치 ubicar correctaemnte y hacer las transferencias de informacion que les sean necesarias.

#### ChordServer

Este tipo servidor se encarga de hacer funcionar el anillo del Chord, para poder buscar los datos distribuidos en el Sistema. Son las compoentes que m치s interacci칩n tienen con los `TweeterServer` ya que estos estar치n constantemente pregunt치ndoles donde almacenar o extraer determinado registro de un dato. Estos son los que poseen la finger table, y la actualizan constantemente. M치s adelante se exlicar치 a mayor profundidad el funcionamiento de los elementos internos de esta componente.

#### TweeterServer
Este servidor es el encargado de trabajar con la base de datos y buscar que todo lo pedido por el usuario sea consistente:

- Que el usuario est칠 loggeado en las acciones que lo requieran
- Que al usuario al que quieran seguir exista
- Que el Dweet que quieran ReDweetear exista
- Que al momento de loggearse sea correcta la informaci칩n de Nick y Contrase침a
- Que al registrarse el Nick de usuario sea 칰nico

Adem치s de guardar en la base de datos los Dweets y ReDweets, nuevos Usuarios, Tokens de usuarios Loggeados, etc. 

Para el manejo de la base de datos utiliza las funciones existentes en el arcivo `view.py`.

Este Servidor se comunica con otros de su mismo tipo con el objetivo de:
>- Pedir informaci칩n sobre la existencia de ciertos datos.
>- Pedirles que guarden datos que les pertenecen.
>- Replicar datos entre los servers dentro de un Nodo.
>- Informar de su llegada al sistema y traspasar datos cuando se agrega un nuevo Nodo.

La comunicaci칩n entre un TweeterServer y un ChordServer se emplea solamente para dos procesos:

>- Cuando la computadora se agrega al sistema y como ChordServer y TweeterServer coexisten en una misma m치quina, atendiendo a peticiones por puertos diferentes, una vez que el ChordServer inserta la computadora en el Nodo correspondiente se informa a s칤 mismo, por el puerto del TweeterServer la informaci칩n necesaria para que este pueda obtener los datos del Nodo al que pertenece. 
>- Cuando un TweeterServer necesita contactar con otro, que tiene los datos de un ususario en espec칤fico; para esto se comunica con el ChordServer desde el otro puerto de la misma computadora y le solicita que haga la b칰squeda en la DHT para conseguirle los IPs de los servidores del Nodo con el que desea contactar.

Con el otro servidor que interactuan los TweeterServer es el EntryServer, el cual es el mediador entre los clientes y los TweeterServer.

>- Los TweeterServer se comunican con los EntryServer para responder a los mensajes de estos inform치ndoles que a칰n est치n activos en la red.
>- Cuando un usuario quiere realizar cualquier acci칩n en Dweeter, el EntryServer con el que conecte es el que se comunica con el TweeterServer y as칤 se maneja la transacci칩n de forma transparente para el usuario.

![](img.png)

# Mecanismos Preliminares Implementados

## Multithreaded server
La atencion a los clientes de cada uno de nuestros servicios tiene en com칰n que siempre est치n esperando conexiones por un puerto determinado y cuando alguien se conecta se le destina un hilo de ejecuci칩n para atenderlo. Nosotros aprovechamos esta situaci칩n encapsulando este comportamiento en la clase MultiThreadedServer. Para especificar el comportamiento del servicio, cuando se construye el objeto se especifica el n칰mero de hilos, el puerto por donde se va a estar escuchando y se le pasa un delegado el cual va a ser el que debe responder al cliente.Este delegado recibe el socket y ser치 ejecutado en su propio hilo. De esta forma logramos abstraer al que vaya a desarrollar un servicio de temas de escucha y threading.

Para nuestros servicios optamos que los clientes sean atendidos en hilos separados y no en procesos separados. La principal motivaci칩n de esto fue por las facilidades que estos permiten para la comunicaci칩n. Como se ver치 m치s adelante nuestros hilos aprovechan las caracter칤sticas de su modelo de memoria para actualizar y leer constantemente objetos compartidos, este comportamiento ser칤a mucho m치s costoso de imitar si hubi칠ramos optado por hacerlo con multiprocesos. A pesar de estas bondades, usar hilos en Python trae sus desventajas, como el hecho de que la concurrencia se ejecuta en un mismo procesador que hace que no se aproveche del todo el paralelismo.

Tener objetos compartidos lleva consigo el riesgo de la ocurrencia de complejos e impredecibles errores de sincronizaci칩n, por lo que usamos varios mecanismos para evitarlos. La base de la mayor칤a de los mecanismos de sincronizaci칩n es el sem치foro el cual consiste en un entero y dos operaciones llamemoslas `release` y `acquire`. Si el entero es mayor que 0 la funci칩n acquire solo decrementa su valor, pero si el entero es cero, el llamado de esta funci칩n provoca que se detenga la ejecuci칩n del hilo que lo llam칩. La funci칩n release incrementa el valor del entero del sem치foro, y si el valor de este antes de ser incrementado era 0 entonces desbloquea a un hilo aleatorio de los que hab칤an sido bloqueados por llamar su `acquire`. 

Usando esta idea tan sencilla se pueden implementar los candados, los cuales son sem치foros en los que el entero puede ser solo `0` o `1`. Con candados se puede lograr el comportamiento de que los hilos accedan los objetos solo uno a la vez. Esto se hace creando un candado para ese objeto y haciendo que todo el que lo vaya a modificar llame `acquire` antes de hacerlo y `release` al haber terminado. Python tiene una implementaci칩n built-in de un candado (`Lock`).

Crear hilos tiene un costo, por lo que normalmente se opta por reutilzarlos. Para lograr esto utilizamos una estructura que se llama **multiconsumer**, **multiproducer queue**. Esta consiste en una cola con capacidad limitada que adem치s de ser thread safe tiene la propiedad de que si se pide un elemento y la cola est치 vac칤a bloquea el hilo de ejecuci칩n de quien se lo pidi칩 y al igual que cuando se va a insertar un elemento y est치 llena. Este comportamiento se puede lograr con dos semaforos, llam칠mosle **P** y **C**. Al comienzo el entero correspondiente a **P** comienza con el valor de la capacidad y el entero de **C** empieza con `0`. Cuando se a침ade un elemento a la cola se llama un `acquire` de **P** y un `release` de **C**, y cuando se saca un elemento se llama un `acquire` de **C** y un `release` en **P**. De esta forma se logra que el entero de **P** sea distinto de `0` ssi a칰n hay capacidad para m치s elementos y el entero de **C** es distinto de `0` ssi a칰n quedan elementos por consumir. Luego se puede construir la piscina de threads haciendo que todos los hilos que queremos utilizar est칠n en un ciclo pidiendo elementos a una cola como estas.

Teniendo en cuenta esto decidimos que el comportamiento del servidor se centre en una **multiproducer, multiconsumer queue**. Vamos a tener un hilo que se dedique a ,constantemente, aceptar conexiones y meter los socket en la cola. Por otro lado vamos a tener `n` hilos constantemente pidi칠ndole `sockets` a la cola para pas치rselo a la funci칩n que proporcion칩 quien construy칩 el server. Cuando se termine la ejecuci칩n de esta funci칩n se va a volver a pedir un socket a la cola.

Tambi칠n con este server proveemos una funcionalidad para terminar de forma segura el servicio. Para esto utilizamos un objeto de `Python` llamado `Event` que entre otras funcionalidades puede servir como una variable booleana y que se puede modificar siendo thread safe. Lo otro que hay que tener en cuenta es que con la cola que usamos tiene la funci칩n de especificar el tiempo que se quiere esperar cuando se pide un elemento y esta est치 vac칤a. Con estas ideas lo que hicimos fue crear un `Event` y pasarle una referencia a todos los hilos, tanto el que est치 esperando conexiones como los que est치n consumiendo de la cola. Luego los hilos que consumen sale de su espera cada cierto tiempo y si la cola est치 vac칤a y el evento que se le pas칩 est치 seteado, salen del ciclo y retornan. De esta forma se asegura que el servidor no termine si tiene a칰n clientes por atender. El hilo que est치 aceptando conexiones puede usar una idea parecida pero este est치 bloqueado por la funci칩n `accept`. Para hacer salir un hilo de un `accept` hay varias ideas, como mandar una se침al a uno mismo. La soluci칩n por la que optamos utiliza la funci칩n `wait` del Event, que en semejanza al candado bloquea el hilo de ejecuci칩n hasta que alguien llama a la funci칩n `set` de ese mismo evento. Lo que hicimos fue poner un hilo cuya 칰nica funci칩n es llamar al wait del event que se le pas칩 a todos los hilos y cuando salga de su espera lo primero que hace es mandar un mensaje vac칤o al localhost, esto desbloquea el `accept` y permite terminar el hilo que estaba escuchando.

## ThreadHolder y State Storage

Nosotros realizamos dos tipos de peticiones, unas que son respondidas inmediatamente, utilizando el mismo socket con el que se mand칩 la petici칩n, y otras que pueden demorar y que pueden llegar desde cualquier servidor. Para implementar las segundas necesitabamos una forma de hacer que el hilo espere hasta que llegue la respuesta para 칠l, pues en un momento determinado puede haber m치s de un hilo esperando por respuestas de este tipo. Esto lo resolvimos asign치ndole un `id` a cada hilo que est칠 esperando por respuesta y haciendo que este espere por un evento creado solo para 칠l. De esta forma se puede enviar la petici칩n con el `id` que se le asign칩 al hilo para que cuando llegue la respuesta esta pueda especificar el `id` del hilo que debe recibirla. Este comportamiento lo encapsulamos en las clases `ThreadHolder` y `StateStorage`. `ThreadHolder` ser칤a el objeto que contiene el `id`, el evento y la variable a la que se le debe asignar la referencia a la respuesta. `StateStorage` es el objeto que se encarga de coordinar todos los `ThreadHolder`. Entre sus funciones principales est치 poder retribuir un `ThreadHolder` dado su `id`, y poder crear nuevos `ThreadHolders` asign치ndole un `id` distinto a los del resto. Usando estos objetos se puede realizar las peticiones que necesitabamos con solo pedir un nuevo `ThreadHolder` al `StateStorage`, enviar la petici칩n con el `id` del `ThreadHolder` y esperar al evento que contiene tambi칠n este objeto. De esta forma cuando llegue la respuesta con el id, se le puede pedir al StateStorage el `ThreadHolder` con este `id`, copiar la referencia a la respuesta en su variable y activar el evento al que estaba esperando el hilo. 

## Chord DHT

Por motivos de escalabilidad la base de datos de nuestro proyecto debe estar distribuida en varios servidores. Para poder trabajar de esta forma necesitamos una forma de encontrar el servidor que contiene la secci칩n de la base de datos que puede responder a nuestras queries. Una soluci칩n a este problema es implementar una **Distributed Hash Table** (`DHT`). 

En nuestro caso nos basamos en **Chord** que es un tipo de DHT. Su idea se basa en asignarle un id a cada servidor y organizarlos en forma de anillo, ordenados en sentido de las manecillas del reloj. Luego, asumiendo que cada dato tiene una llave num칠rica, este va a estar almacenado en la secci칩n de la base de datos del servidor que cumple que su `id` es mayor o igual a la llave del dato y el de su antecesor es menor estricto, a partir de ahora esto va ser equivalente a decir que ese servidor es sucesor de esa llave. El nodo de menor `id` es un caso especial pues su antecesor es el m치ximo y siempre es mayor que 칠l. Por esto decidimos que al m칤nimo pertenece el dato cuyo `id` es menor que el del m칤nimo o mayor que el del  m치ximo.

Por motivos tambi칠n de escalabilidad un nodo de Chord no debe tener que estar al tanto de la existencia de todos los nodos que participan en la `DHT`. Si cada nodo conociese el id y la direccion IP de su sucesor y su predecesor entonces cualquier nodo pudiera resolver qui칠n tiene cierto dato recorriendo el anillo. Pero esta estrategia es bastante lenta considerando que probabil칤sticamente la mitad de las peticiones tendr칤an que ser redirigidas a trav칠s de la mitad del anillo para ser resueltas.

Existe una forma de hacerlo con un mejor equilibrio entre nodos que necesita conocer un nodo y redirecciones por las que necesita pasar una petici칩n. Se puede almacenar tantos nodos como  parte entera por debajo del logaritmo en base 2 del mayor `id` posible y que estos cumplan que el `i`-칠simo sea el sucesor de id $+ 2^{i - 1}$. A la tabla con estos nodos se le conoce como Finger Table y ahora la petici칩n en vez de redirigirsela al sucesor, se redirige al nodo con mayor `id` que sea menor que la llave del dato. Solo se le redirige la petici칩n a alguien con `id` mayor cuando es el sucesor, que en este caso se habr칤a encontrado a quien posee el dato. Esta forma de buscar es correcta porque siempre avanza y a alguien que es menor que la llave al no ser que est칠 seguro de que posea el dato y se mantiene la propiedad de que solo necesita que el sucesor y el predecesor est칠n correctos para que la b칰squeda correctamente.

Nuestra DHT est치 pensada para ser construida de manera incremental. Para insertar un nuevo nodo `N` solo se necesita que el nodo que va a ser el predecesor de N lo ponga como su sucesor en la finger table y que el nodo que va a ser su sucesor ponga a N como su predecesor. Cuando se crea el nodo se le asigna un ID, en nuestro caso el `SHA256` de su propio IP. Luego, asumiendo que la `DHT` est치 funcionando correctamente, se le pregunta a un nodo cualquiera por el servidor que es sucesor del ID que se le asign칩 a `N`. Como el predecesor de este sucesor es tambi칠n el predecesor del nuevo nodo todo el proceso de inserci칩n ocurre entre estos tres. El nuevo nodo le avisa a su sucesor que 칠l es su nuevo predecesor, a lo que este le responde con el `id` y el IP de su predecesor. Luego le dice al predecesor que 칠l es su nuevo sucesor, haciendo que este modifique su **finger table**. Para terminar se le vuelve a escribir al sucesor para confirmar que ya el predecesor lo tiene como sucesor haciendo que este lo ponga como su predecesor en su figer table. Las finger table del resto de los nodos est치 en un estado que permite que las b칰squedas funcionen bien porque como se hizo notar antes esto solo depende de que el predecesor y el sucesor est칠n correctos, y los que deb칤an cambiar ya fueron cambiados. Pero se necesita que el nuevo nodo se tenga en cuenta en las finger table del resto para que las redirecciones sean m치s eficientes, por lo que cada nodo actualiza su tabla peri칩dicamente preguntando por el sucesor del valor correspondiente a cada fila. Como esta b칰squeda funciona correctamente, las filas de las finger table a las que le corresponda el nuevo nodo van a ser corregidas eventualmente.

Lo que hemos mostrado hasta ahora explica el funcionamiento de la mayor칤a de los casos. Pero como un nodo puede resolver qui칠n posee un dato con una llave menor que la de 칠l y su predecesor. Todo nodo tiene conocimiento de su predecesor inmediato y se podr칤a recorrer,pero esto implicar칤a una busqueda lineal. Otra alternativa es redirigir la petici칩n al m칤nimo pues si la llave sigue siendo menor que 칠l, es de 칠l y si es mayor con las finger table se puede buscar eficientemente. Pero hacer que todos los nodos lleven la cuenta de qui칠n es el m칤nimo en este momento tiene sus complicaciones, pues en este caso ser칤a otro factor que influye en la correctitud. La alternativa que encontramos es hacer que todos los nodos respondan por su `id` y por `id` $+$ `MAX`, donde `MAX` es el mayor `id` posible. De esta forma por ejemplo el sucesor inmediato del m치ximo es el m칤nimo pero con el `id` + `MAX`. Ahora cuando un nodo quiera encontrar a alguien que tiene un `id` menor que 칠l solo tiene que preguntar por el sucesor de la llave del dato sumado a `MAX`.

# Comunicaci칩n de Componentes


Para la comunicaci칩n entre las componentes del sistema se cre칩 un protocolo de comunicaci칩n. Cada mensaje enviado en la red tiene la estructura [ Tipo | Protocolo | Datos ], de modo que cada Server pueda distinguir bas치ndose en el `Tipo` del Server que le escibe y en el `Protocolo` del mensaje, cu치les son las acciones a realizar y cuales son los valores que est치n almacendaos en los `Datos`. 

Los tipos disponibles son: `Client, Entry, Logger, Tweet, Chord`

uno por cada Servidor que interviene,excepto los TweeterServer que por cuesti칩n de comodidad responden tanto como tipo Logger como por tipo Tweet, para separar las funcionalidades de registro y loggeo del resto de manejo de Dweets y ReDweets.


## Consultas

El mecanismo base para hacer una consulta completa desde el cliente pasa por una serie de pasos:

- El `Client` realiza la petici칩n con un `EntryServer`, y mantiene la comunicaci칩n abierta hasta que se le responda.
- El `EntryServer` le escribe a alg칰n `TweeterServer` con la solicitud de la consulta que desea realizar, y cierra la conexi칩n.
- El `TweeterServer` le pide a su `ChordServer` correspondiente el ip del `TweeterServer` que debe tener la informaci칩n solicitda y cierra la conexi칩n.
- Los `ChordServer` realizan la b칰squeda con el algoritmo de Chord y al encontrar al indicado, se le escribe nuevamente al `TweeterServer` inicial, con el IP del servidor que puede responder la consulta, y se cierra la conexi칩n.
- Este `TweeterServer` le escribe al que tiene los datos que necesita, haciendo al solicitud, y cierra la conexi칩n.
- Al realizar la consulta, se le responde abriendo una nueva conexi칩n.
- Luego el `TweeterServer` inicial le responde por una nueva conexi칩n, al `EntryServer` con los datos solicitados.
- Y finalmente el `Client` recibe la respuesta de la consulta.

Debemos aclarar algunas cosas de este proceso. La comunicaci칩n entre componentes NO `Client`, tiene sentido que se hagan abriendo y cerrando una conexi칩n para preguntar, y hacer lo mismo para responder. Por qu칠? Pues, es un puerto menos utilizado que estar치 esperando una respuesta; al realizar la b칰squeda con el Chord se formar칤a un arco de conexiones abiertas, que deber칤an esperar una respuesta por la misma conexi칩n, y esto obligar칤a a dejar las conexiones mucho m치s tiempo abiertas, y en el caso de que una de estas intermedias se rompiera, la respuesta final no llegar칤a al `TweetServer`; y adem치s hay consultas donde el `TweetServer` necesita hacer varias peticiones en vez de una sola. Por otra parte la conexi칩n entre `Client` y `EntryServer` tiene sentido dejarla abierta ya que el Server no tendr칤a forma de comunicarse con el cliente abriendo una nueva conexi칩n, ya que para hacer esto, los `Client` deber칤an entonces tamb칤en estar escuchando por un puerto continuamente, y esta funci칩n la consideramos innecesaria de momento.

Cosas importantes a tener en cuenta, es que cuando se realiza una consulta, tanto el `EntryServer` como el `TweeterServer` esperan un tiempo de seguridadpara recibir la respuesta, y si no la reciben antes de este tiempo, notifican un error. De esta manera se garantiza que los hilos con los que se realizan las peticiones no queden flotando e inutiliz치ndose.

## Stalking

Este proceso consiste en "preguntar continuamente" a otra componente si est치 disponible en la red. La utiliza principalmente el `EntryServer`, para reconocer a los otros `EntryServer`y los `ChordServer` que est치n vivos. El funcionamiento es bastante sencillo:

> Cada cierto tiempo aleatorio (con probabilidad uniforme) el `EntryServer` manda un mensaje con protocolo `ALIVE_REQUEST` a estos sevidores, esperando una respuesta, la cual de ser recibida, se actualiza el 칰ltimo tiempo de vida de dicha componente. En caso que no se reciba respuesta, este tiempo de vida no se actualiza, y poco tiempo despu칠s, por lo general, luego de 3 veces sin tener contacto con la componente, se considerar치 "muerta".

Considerar una componente "muerta" tiene una interpretaci칩n diferente para cada tipo de componente.

- En el caso de que un `ChordServer` se considere muerto por un `EntryPoint`, implica que cuando otro `ChordServer` se quiera introducir al anillo del Chord, el `EntryPoint` NO le recomendar치 el IP del `ChordServer` muerto, sino aleatoriamente otros que est칠n vivos. Adem치s como en la implementaci칩n del Sistema, los Servidores `TweeterServer` y `ChordServer` se montan sobre la misma PC, sirve tambi칠n para no incluir una componente `TweeterServer` ca칤da, al solicitar servicios generales entrantes del `Client`, como iniciar sesi칩n, ver perfil, etc...
- En el caso de que un `EntryPoint` sea el muerto, entonces se a침ade a la `Lista de Tareas` de este (luego se ver치 mejor), enviar los IPs de los `ChordServer` que el vivo tiene, una vez que el `EntryPoint` muerto reaparezca.

Note que el principal beneficio de este proceso de `Stalking` es que se evita los fallos producidos cuando una PC sale del Sistema. Pero en contraposici칩n se genera conexiones extras en el red, lo cual podr칤amos pensar que recargar칤a esta; sin embargo note que al acosar a una componente se hace en luego de un tiempo random, lo cual posibilita que la red realmente no se sobrecargue, al menos en la mayor칤a del tiempo.

## Tareas Pendientes

Este proceso consiste en disponer de una lista de acciones que la componente actual debe realizar con otra componente, y debido a alg칰n motivo no pudo hacerla en un comienzo pero tiene la necesidad de hacerla en alg칰n momento. Adem치s como parte de este proceso, est치 el hecho de enviar estas tareas. Por lo general quienes utilizan una lista de tareas pendientes son el `EntryServer` y el `TweetServer`.

En s칤 el proceso consite un poco m치s a detalle en:
> Llevar un diccionario con los IPs de las componentes a las que posiblemente se les podr칤a enviar una acci칩n retrasada, y su lista de acciones retrasadas. Cada cierto tiempo aleatorio (probabilidad uniforme) se env칤an las tareas retrasadas. Si estas llegan correctamente, se van eliminando del diccionario.

El tipo de tarea pendiente var칤a seg칰n la componente que emite la acci칩n:

- En el caso de los `EntryServer`, las tareas pendientes, van m치s relacionadas con agregar un `TweeterServer` que se insert칩 en el anillo del Chord, para notificar al resto de `EntryServer`, que hay un nuevo IP v치lido de `TweeterServer` y/o `ChordServer` al que se le puede hacer alguna petici칩n. Algo importante, es que 

- En el caso de los `TweeterServer` sus tareas pendientes son internas con las r칠plicas dentro del Nodo. En este caso cuando un `TweeterServer` agregue una info nueva, est치 se a침adir치 a sus tareas pendientes, y luego de un rato, se enviar치 al destinatario final.

Note que la lista de tareas de esta forma eventualmente actualizar치 a las componentes finales. En cuanto a la sobrecarga que pueda provocar en la red es similar a  la del `Stalking`, donde las peticiones no se realizan todas a la vez, sino entre tiempos aleatorios.

## Inserci칩n de un nuevo `ChordServer-TweeterServer`

Cuando un nuevo `ChordServer-TweeterServer` se quiere insertar en el anillo del Chord, primero estable una comunicaci칩n con alg칰n `EntryServer` para pedir alg칰n `ChordServer` que ya est칠 en el anillo del Chord. Luego le pide a este le pide el ip del servidor que es sucesor del id que se le asign칩. Luego este le avisa al sucesor que es su nuevo predecesor y al predecesor de su sucesor que es su nuevo sucesor, quedando insertado en sus fingertable, y por supuesto construye la suya con este sucesor y el otro predecesor. Finalmente cuando el Chord se inserta se comunica con el `EntryServer` para notificarle que se insert칩 correctamente, al igual que se comunica con su `TweeterServer` asociado para indicarle el id que le corresponde, sus sucesores, y sus hermanos.

## R칠plicas y Transferencia de Datos

Cuando un nuevo `TweeterServer-ChordServer` se inserta al anillo del Chord este en un inicio debe saber c칩mo quiere insertarse, si como nuevo Nodo, o como r칠plica en otro nodo. En cualquiera de ambos casos por lo general ocurr칤ra una trnasferencia de muchos datos, ya que:

- En el caso de insertarse como nuevo nodo, deber치 copiar los datos que le corresponden en su rango de hash asignado.
- En el caso de insertarse como r칠plica dentro de un nodo, deber치 replicar todos los datos que tenga el nodo.

Por este motivo, una vez insertado al anillo, la parte `ChordServer` le informa a la `TweeterServer` de ese ordenador mediante un mensaje con los datos `chord_id`, `succesors` y `siblings`, que son el identificador que tienen dentro del anillo, una lista con los IPs de sus sucesores en el anillo y una lista con los IPs de las otras m치quinas de su propio Nodo(hermanos), respectivamente, esta 칰ltima ser치 vac칤a en el caso que sea el primero de ese Nodo. Bas치ndose en esta informaci칩n, el `TweeterServer` establecer치 comunicaci칩n con alguno de sus hermanos en caso de tener, o con alguno de sus sucesores en caso contrario, creando un nuevo hilo para dicha comunicaci칩n.


Para realizar una transferencia de datos correcta, el orden en que se transfieran las tablas de la base de datos importa, debido a que los usuarios sun llaves for치neas en la mayor칤a de la tablas, la tabla de ususarios ha de ser la primera en copiarse. De este modo se van transfiriendo bloques de datos entre una computadora y la otra hasta que se hayan enviado toda la base de datos.

>Para la transferencia de datos se requiere que el servidor nuevo env칤e su `chord_id` para que as칤 el otro servidor sepa que porci칩n de la base de datos enviar. 
Los datos se transfieren en bloques de 20 filas de la tabla cada vez.

# Tolerancia a Fallas

Un sistema distribuido por lo general puede presentar 3 tipos de fallas: las transitorias, las permanentes, y las intermitentes. A continuaci칩n explicamos las tolerancias con las que cuenta nuestro sistema distribu칤do:

- Lectura correcta de mensajes: Todo informaci칩n que se env칤e debe respectar el protocolo de comunicaci칩n explicado. Si por alguna extra침a raz칩n los bytes transferidos se modificaran y llegaran con alg칰n defecto, los componentes al recibirlos e intentar decodificarlos, lo notar치n y lo descartar치n notificando por lo general un error en la conexi칩n. Esto nos ayudar치 a que una falla intermitente de eeste estilo, mate hilos de ejecuci칩n.

- Env칤o M칰ltiple: En la mayor칤a de las comunicaciones las componentes implicadas en el env칤o de informaci칩n contar치n con una peque침a lista de IPs a los que pudiera solicitar la consulta. Entonces en vez de establecer la comunicaci칩n solamente con una componente, esta intentar치 establecer una conexi칩n con cada uno, y con el primero que esta conexi칩n se establezca, se realizar치 la consulta. Es importante aclarar que NO es que se intenten abrir de forma simult치nea todas estas conexiones sino que se van probando uno a una, hasta que la primera acierte. Note que esto disfraza algunas las fallas transitorias cuando no se puede establecer conexi칩n con alguna componente, y el cliente por otra parte no se entera con cuantos servidores no se pudo estableceruna comunicaci칩n.

- Env칤o persistente: Hay casos donde es necesario establecer una comunicaci칩n con una componente espec칤fica y que a칰n cuando no est칠 disponible, se le env칤e al reincorporarse al sistema. Ejemplo de ello son las `Tareas Pendientes`. En sentido general tenemos 2 categor칤as para esta tolerancia:
    - Relajado: Es cuando el recurso o el mensaje que se quiere enviar no requiere de una prioridad sumamente alta, y por lo tanto se intenta reenviar luego de un tiempo no tan corto (generalmente aleatorio). D칤gase por ejemplo, datos que debe actualizar una r칠plica; en este caso, es necesario que la r칠plica en cuesti칩n agregue esa informaci칩n, pero por lo general si esta conexi칩n no se establece es porque dicha r칠plica est치 ca칤da, as칤 que debe tomar su tiempo reincorporarla, por lo tanto no hay necesidad de insistir con una frecuencia muy alta. Y de esta forma se van realizando otras tareas que s칤 se pueden realizar.
    - Frencuente: Es cuando el recurso o el mensaje que se quiere enviar tiene un alto grado de prioridad, por ejemplo la actualizaci칩n de la DHT del anillo del Chord. El 칰nico factor del que depende el funcionamiento correcto de Chord es que el sucesor y el predecesor puestos en la finger table est칠n correctos. Por lo que al insertarse un nodo en el Chord este lo intenta persistentemente, pues de quedarse a medias el proceso el predecesor de su sucesor y el sucesor de su predecesor puede quedar en un estado incorrecto.

    De esta manera note que los fallos temporales al sacar alguna r칠plica del Sistema, y luego integrarlas se evitar칤an.

- R칠plica de informaci칩n en un nodo: Relacionado con el punto anterior, si ocurre un fallo permanente donde una r칠plica sale del Sistema, digamos que porque dicha PC se rompi칩, se le quemaron los discos, etc, pudiera reemplzarse por otra, que al a침adirla nuevamente al nodo obtendr칤a toda la informaci칩n que se hab칤a perdido, ya que dentro de un nodo se replica la informaci칩n. De esta manera note que el usuario no se entera cu치ndo desapareci칩 una r칠plica por este causa, y tampoco cu치ndo se incorpora una nueva.

- Sugerencia de Componentes Vivas: Ya explicamos en secciones anteriores c칩mo funciona el proceso de `Stalking`, pero debemos se침alar que este tiene un papel importante para evitar fallas transitorias de errores en la red para comunicarse con alguna componente, pues cuando un `ChordServer` le pide a un `EntryPoint` alg칰n IP de alguien en el anillo, para poder incorporarse, este le da aleatoriamente una lista peque침a de `ChordServer`s que fueron acosador recientemente y se suponen que est칠n vivos. De esa forma es m치s probable que se logre establecer una conexi칩n.

- Tiempo de Espera: Cuando se hace una petici칩n entre componentes cuya respuesta no ser치 inmediata, por lo general se toma un tiempo para procesar la consulta entera. Adem치s habr치 veces donde luego de hacer alguna petici칩n se pudiera romper la conexi칩n antes de recibir una respuesta. Por lo tanto una forma de evitar provocar un error mayor, como dejar trabajando un hilo innecesariamente o incluso que pueda quedarse siempre flotando inutilizado, es esperar un tiempo la respuesta de la consulta. Si la respuesta llega a tiempo, el flujo de la operaci칩n que se est칠 realizando contin칰a, pero en caso de que el tiempo de espere se sobrepase, se para la operaci칩n y se notifica que se agot칩 el tiempo de espera. De cara al cliente esto es de lo 칰nico que se puede enterar para peticiones que vengan desde el `Client`, pero ni tan siquiera sabr치 si fue que una r칠plica se cay칩, u otra cosa. En dicho caso, el usuario podr칤a repetir la consulta.

## Transparencia de las operaciones

Cuando un cliente accede a cualquiera de las funcionalidades de Dweeter, todas las acciones que el sistema realice se mantienen transparentes para 칠l. La 칰nica informaci칩n que conoce un usuario es que se est치 comunicando con un servidor al que le est치 haciendo peticiones y que este le env칤a respuestas, pero jam치s contacta siquiera con los `ChordServer-TweeterServer`, ni conoce que su informaci칩n est치 fragmentada en distintas computadoras de la red. 

## Cach칠 del Cliente

Adem치s de todas las funcionalidades que brinda el Sistema Distribuido se implement칩 del lado del `Cliente` un `ShellCLient` interactivo para que el usuario pueda estar dentro de `Dweeter`. Esta implementaci칩n adem치s cuenta con una memoria `Cach칠` de la informaci칩n de perfiles. Esta se va llenando a medidad que se van visitando perfiles, o pidiendo nuevos Dweet y/o ReDweets; y finalmente cuando no haya conexi칩n se le mostrar치 al usuario publicaciones guardadas en ellas. De esta forma el usuario puede entretenerse con publicaciones que aunque ya las vio, puede analizarlas mejor y no tener una pantalla en blanco, mientras regresa la conexi칩n.